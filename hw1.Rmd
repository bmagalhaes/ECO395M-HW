---
title: "homework1"
author: "Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim"
date: "`r format(Sys.Date())`" 
output:
  md_document:
    variant: markdown_github
---
By Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(FNN)
urlfile<-'https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW/master/sclass.csv'
sclass<-read.csv(url(urlfile))
```

# Exercise 1.2
We used K-nearest neighbors to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65 AMG.  In order to do this, we divided our data into 2 subgroups, 350 and 65 AMG, and got rid of all the other data.

```{r 1.2.1, echo=FALSE}
# Divide groups into 2 and define 'Root Mean Squre Error' function
sclass_350 <- subset(sclass,(sclass$trim == "350"))
sclass_65AMG <- subset(sclass,(sclass$trim == "65 AMG"))
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```

### Sclass 350
First, we'll look on the Sclass 350 data. We can see there's a negative relationship between mileage and price plotted as below  


```{r 1.2.2, echo=FALSE}
# plot the data
p_350 = ggplot(data = sclass_350) + 
  geom_point(mapping = aes(x = mileage, y = price), color='medium purple')+
  theme_bw()+
  labs(title = "Mileage vs Price (Sclass 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_350
```

And we splitted Sclass 350 data into two groups. One is "training set", and the other is "test set". The training set accounts for 80% of whole data. 

```{r 1.2.3, echo=FALSE}
# Make a train-test split
N = nrow(sclass_350)
N_train = floor(0.8*N)
N_test = N - N_train

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = sclass_350[train_ind,]
D_test = sclass_350[-train_ind,]

# optional book-keeping step:
D_test = arrange(D_test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

```

Then we ran K-nearest-neighbors for k, starting from k=3 to higher value. We faced an error when k=2, so the possible minimum value of k was 3. The fitted model for k=3 is as below, and RSME is 10984.7.
  
```{r 1.2.4, echo=FALSE}
# k=3
knn3 = knn.reg(train = X_train, test = X_test, y = y_train, k=3)
ypred_knn3 = knn3$pred

D_test$ypred_knn3 = ypred_knn3

p_test_knn3 = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  geom_path(mapping = aes(x = mileage, y = ypred_knn3), color='red') +
  theme_bw()+
  labs(title = "Fitted Model for k=3 (Sclass 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_knn3
rmse(y_test, ypred_knn3)
```

When k=20, The fitted model is as below. RSME is 9636.9, which is smaller than RSME when k=3. 


```{r 1.2.5, echo=FALSE}
# k=20
knn20 = knn.reg(train = X_train, test = X_test, y = y_train, k=20)
ypred_knn20 = knn20$pred

D_test$ypred_knn20 = ypred_knn20

p_test_knn20 = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  geom_path(mapping = aes(x = mileage, y = ypred_knn20), color='red') +
  theme_bw()+
  labs(title = "Fitted Model for k=20 (Sclass 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_knn20
rmse(y_test, ypred_knn20)
```

Now, the fitted model for k=100 below shows us the fact that the graph gets smoother as k goes bigger. But RSME when k=100 is 11988.9, which is bigger than that of when k=20. So it is probable that the optimal k that minimizes RSME will be somewhere between k=3 and k=100.


```{r 1.2.6, echo=FALSE}
# k=100
knn100 = knn.reg(train = X_train, test = X_test, y = y_train, k=100)
ypred_knn100 = knn100$pred

D_test$ypred_knn100 = ypred_knn100

p_test_knn100 = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  geom_path(mapping = aes(x = mileage, y = ypred_knn100), color='red') +
  theme_bw()+
  labs(title = "Fitted Model for k=100 (Sclass 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_knn100
rmse(y_test, ypred_knn100)
```

In order to find the optimal k, we plotted k versus RSME for every k. The graph below shows that RSME is minimized to 9487.9 when k equals 10.


```{r 1.2.7, echo=FALSE}
KNN_result <- data.frame(K=c(), rsme=c())
# KNN
for(v in c(3:nrow(X_train))){
  knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=v)
  ypred_knn = knn_K$pred
  KNN_rsme = rmse(y_test, ypred_knn)
  KNN_result <- rbind(KNN_result,c(v,KNN_rsme))
}

colnames(KNN_result) = c("K","RSME")
Kmin = KNN_result$K[which.min(KNN_result$RSME)]

P_KNNresult_350 = ggplot(data = KNN_result)+
  geom_line(aes(x = K, y = RSME))+
  geom_line(aes(x = Kmin, y = RSME), col = "red")+
  theme_bw()+
  labs(title = "K vs RSME(Sclass 350)")+
  theme(plot.title = element_text(hjust = 0.5))
P_KNNresult_350
Kmin
knn10 = knn.reg(train = X_train, test = X_test, y = y_train, k=10)
ypred_knn10 = knn10$pred
D_test$ypred_knn10 = ypred_knn10
rmse(y_test, ypred_knn10)
```


```{r 1.2.7, echo=FALSE}
