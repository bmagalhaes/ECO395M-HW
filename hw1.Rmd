---
title: "homework1"
author: "Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim"
date: "`r format(Sys.Date())`" 
output:
  md_document:
    variant: markdown_github
---
# Bernardo Arreal Magalhaes - UTEID ba25727
# Adhish Luitel - UTEID al49674
# Ji Heon Shim - UTEID js93996

# Exercise 1

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(FNN)

#setup
urlfile<-'https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW/master/abia.csv'
abia<-read.csv(url(urlfile))
```

## Exercise 1.1
In this question, we have all the data of flight information that arrive at or depart from Austin-Berstrom International Airport.  

```{r 1.1.1, echo=FALSE}
abia = mutate(abia, category = ifelse(Origin == "AUS", "Departure", "Arrival"))
abia$CRSTWindow_D = floor(abia$CRSDepTime/100)
abia$CRSTWindow_A = floor(abia$CRSArrTime/100)
us_states <- map_data("state")
urlfile<-'https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW/master/airports.csv'
airports = read.csv(url(urlfile))
airports = subset(airports,!(airports$iata_code == ""))
airports = airports[-c(1:4,7:13,15:18)]

abia_conf <- subset(abia,(abia$Cancelled == "0"))
abia_general <- abia_conf
abia_cancel <- subset(abia,(abia$Cancelled == "1"))

```

### Which airline company has the highest cancellation rate?
First, we'll have a close look on cancellation rates by airline companies. Our data shows that American Eagle Airline (MQ), the regional brand of American Airlines, has the highest cancellation rate among all the airlines flying from Austin.

```{r 1.1.2, echo=FALSE, warning=FALSE}
departures_fly <- subset(abia_conf, abia_conf$category == "Departure")
Arrivals_fly <- subset(abia_conf, abia_conf$category == "Arrival")
departures_fly = mutate(departures_fly, delay = ifelse(DepDelay <= 0, 0, DepDelay))
delay_summ = departures_fly %>%
group_by(Dest)  %>% 
summarize(dly.mean = mean(delay), na.rm=TRUE)
  
delay_summ = left_join(delay_summ, airports, by = c("Dest" = "iata_code"))
delay_summ = mutate(delay_summ,AUSLat = 30.194500)
delay_summ = mutate(delay_summ,AUSLong = -97.66990)

cancel_rate = abia %>%
  group_by(UniqueCarrier)  %>%  # group the data points by model nae
  summarize(cancelrate = length(which(Cancelled == 1)) / (length(which(Cancelled == 1)) + length(which(Cancelled == 0))))
cancel_rate = mutate(cancel_rate,cancelrate = cancelrate*100)

ggplot(cancel_rate, aes(x=reorder(UniqueCarrier, cancelrate), y=cancelrate)) + 
  geom_bar(stat='identity') + 
  coord_flip() +
  labs(title = 'Cancellation Rate per Carrier', x = "Carrier", y = "Cancellation Rate (%)")+
  scale_size_area()+
  theme(plot.title = element_text(hjust = 0.5))
```

### Which destination is likely to cause the longest departure delay?
Next, we examined all the flights departing from Austin, and arranged departure delays by their destinations. Our analysis shows that DSM (Des Moines International Airport - Iowa) is the destination which causes the most departure delay from Austin. It is important to notice that there was only one flight to this destination during the entire year of 2018, what explain such a discrepancy from the other destinations.

```{r 1.1.3, echo=FALSE, warning=FALSE}
ggplot(delay_summ, aes(x=reorder(Dest, dly.mean), y=dly.mean)) + 
  geom_bar(stat='identity') + 
  coord_flip() +
  labs(title = 'Average Departure Delay per Destination', x = "Airport", y = "Minutes") +
  scale_size_area()+
  theme(plot.title = element_text(hjust = 0.5))
```

### What time of day does flight delay mostly occur?
Next, we arranged our departure delay data by scheduled departure time. Our analysis shows that departure delays are most likely to happen between 0 to 1 o'clock at night. We don't know the exact explanation for this, but among possible conjectures we think it might be due to the fact that late night flights "carry-on" the delays acumulated during the day since it operates with the aircrafts that are arriving from other destinations. It might be the reason why late-night flights are sold by lowest fares.

```{r 1.1.4, echo=FALSE}
# plot the summ for all the airline companies 
abia_conf = mutate(abia_conf, DepDelay = ifelse(DepDelay <= 0, 0, DepDelay))

abia_CRSsumm_D_total <- departures_fly %>%
  group_by(CRSTWindow_D) %>%
  summarise(DepDelay_mean = mean(delay))

abia_CRSsumm_A_total <- Arrivals_fly %>%
  group_by(CRSTWindow_A) %>%
  summarise(ArrDelay_mean = mean(ArrDelay, na.rm=TRUE))

ggplot(data = abia_CRSsumm_D_total)+
geom_bar(aes(x = CRSTWindow_D, y = DepDelay_mean),stat='identity',position='dodge')+
labs(title = "Scheduled Time vs Average Departure Delay", x = "Scheduled Time", y = "Departure Delay")+
theme_bw()+
theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank())
```

Now, we arranged our arrival delay data by scheduled arrival time. Our analysis shows that arrival delays are most likely to happen between 22 to 23 o'clock at night. Again, we can reasonably argue that it might be due to the "carry-on" effect of the delays acumulated during the day.
```{r 1.1.5, echo=FALSE}
ggplot(data = abia_CRSsumm_A_total)+
geom_bar(aes(x = CRSTWindow_A, y = ArrDelay_mean),stat='identity',position='dodge')+
labs(title = "Scheduled Time vs Average Arrival Delay", x = "Scheduled Time", y = "Arrival Delay")+
theme_bw()+
theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank())
```

### Which month of year does flight delay mostly occur?
This time, we'll extend our view from day to year to find out the month which flight delay mostly occurs. In case of departure delays from Austin, December is the worst month if someone wants to avoid any delays. If you depart from Austin in December, you'll expect more than 15 minutes of delay on average - what matches with common sense that the worst time of the year to fly is during holidays season.

```{r 1.1.6, echo=FALSE}
# plot the summ for all the airline companies 
abia_summ_Month <- abia_conf %>%
  group_by(Month,category) %>%
  summarise(DepDelay_mean = mean(DepDelay),ArrDelay_mean = mean(ArrDelay, na.rm=TRUE))

ggplot(data = subset(abia_summ_Month,(abia_summ_Month$category == "Departure")))+
geom_bar(aes(x = Month, y = DepDelay_mean),stat='identity',position='dodge')+
labs(title = "Month vs Average Departure Delay", x = "Month", y = "Departure Delay")+
theme_bw()+
scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12),labels = c("1" = "Jan", "2" = "Feb", "3" = "Mar", "4" = "Apr", "5" = "May", "6" = "Jun", "7" = "Jul", "8" = "Aug", "9" = "Sep", "10" = "Oct", "11" = "Nov", "12" = "Dec"))+
theme(plot.title = element_text(hjust = 0.5))
```

December is the worst month again to avoid delays in case of arrival, too. It is likely to be caused by an increase of flight demands at the end of year.

```{r 1.1.7, echo=FALSE}
ggplot(data = subset(abia_summ_Month,(abia_summ_Month$category == "Arrival")))+
geom_bar(aes(x = Month, y = ArrDelay_mean),stat='identity',position='dodge')+
labs(title = "Month vs Average Arrival Delay", x = "Month", y = "Arrival Delay")+
scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12),labels = c("1" = "Jan", "2" = "Feb", "3" = "Mar", "4" = "Apr", "5" = "May", "6" = "Jun", "7" = "Jul", "8" = "Aug", "9" = "Sep", "10" = "Oct", "11" = "Nov", "12" = "Dec"))+
theme_bw()+
theme(plot.title = element_text(hjust = 0.5))
```

### Which airline company shows the longest delay time?

Now, we'll see which airline company shows the longest delay time. In case of departure delays, ExpressJet Airlines (EV) shows the worst performance among all the airlines. The boxplot below shows that EV has the largest median and variance. And the barplot also shows that average departure delay time is the longest in EV.

```{r 1.1.8, echo=FALSE}
bwplot(DepDelay~UniqueCarrier, data=abia_general, ylim=c(-30,60)
, xlab="Airlines", ylab="Departure delays(min)", main="Departure delays by Airlines")
AvgDepDelay_Unique=abia_conf%>% group_by(UniqueCarrier) %>% summarize(AvgDepDelay= mean(DepDelay) )

mean.abia_conf<-as.data.frame(tapply(abia_conf$DepDelay, abia_conf$UniqueCarrier, mean))
mean.abia_conf$UniqueCarrier<-rownames(mean.abia_conf)
names(mean.abia_conf)<-c("DepDelay","UniqueCarrier")
ggplot(mean.abia_conf, aes(reorder(UniqueCarrier, -DepDelay, sum), DepDelay))+geom_bar(stat="identity") +
labs(title = "Carrier vs Average Departure Delay", x = "Carrier", y = "Minutes")+
theme_bw()+
theme(plot.title = element_text(hjust = 0.5))
```

In case of arrival delays, the box plot indicates us that PSA Airlines (OH) has the the highest median and also unstable delay range by large variance. The barplot below confirms this idea by showing the largest average arrival delay time for OH.

```{r 1.1.9, echo=FALSE}
bwplot(ArrDelay~UniqueCarrier, data=abia_conf, ylim=c(-60,70), xlab="Airlines", ylab="Arrival delays(min)", main="Arrival delays by Airlines")
AvgDepDelay_Unique=abia_conf%>% group_by(UniqueCarrier) %>% summarize(AvgArrDelay= mean(ArrDelay) )

mean.abia_conf<-as.data.frame(tapply(abia_conf$ArrDelay, abia_conf$UniqueCarrier, mean, na.rm=TRUE))
mean.abia_conf$UniqueCarrier<-rownames(mean.abia_conf)
names(mean.abia_conf)<-c("ArrDelay","UniqueCarrier")
ggplot(mean.abia_conf, aes(reorder(UniqueCarrier, -ArrDelay, sum), ArrDelay))+geom_bar(stat="identity") +
labs(title = "Carrier vs Average Arrival Delay", x = "Carrier", y = "Minutes")+  theme_bw() +
theme(plot.title = element_text(hjust = 0.5))

```

### Suplemental analysis per destination.

When focusing on the most common destinations for flights departing from ABIA, we can see that Dallas, Houston, Phoenix and Denver concentrate approximately 40% of the flights leaving Austin.

```{r 1.1.10, echo=FALSE}

dest_traf = departures_fly %>%
  group_by(Dest)  %>% 
  summarize(number = n())
dest_traf = left_join(dest_traf, airports, by = c("Dest" = "iata_code"))
dest_traf = mutate(dest_traf,AUSLat = 30.194500)
dest_traf = mutate(dest_traf,AUSLong = -97.66990)
number_quant = subset(dest_traf,dest_traf$number >= 939)

ggplot() +
theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank(),  panel.grid.minor = element_blank())+
geom_polygon(data = us_states,  aes(long, lat, group = group), fill = "grey", col = "black") +
geom_curve(data=number_quant, aes(x = AUSLong, y = AUSLat, xend = longitude_deg, yend = latitude_deg, colour = number), size = 1, curvature = 0.1) + 
geom_point(data=number_quant, aes(x=longitude_deg, y=latitude_deg),color="red",size=1) +
scale_colour_gradient2(low="blue", high="red")+
labs(title = 'Number of Flights per Destination - 3rd Quantile', x = "", y = "")+
theme(plot.title = element_text(hjust = 0.5))+
scale_x_discrete()+
scale_y_discrete()
```

When focusing on destinations that show a higher delay on average, we observe that the pattern changes. Among possible conjectures, we believe that bigger delays might occur when there is a less frequent operation instead of a well established routine.

```{r 1.1.11, echo=FALSE}
delay_quant = subset(delay_summ,delay_summ$dly.mean >= 12.771)
ggplot() +
  theme(plot.title = element_text(hjust = 0.5), panel.grid.major = element_blank(),  panel.grid.minor = element_blank())+
  geom_polygon(data = us_states,  aes(long, lat, group = group), fill = "grey", col = "black") +
  geom_curve(data=delay_quant, aes(x = AUSLong, y = AUSLat, xend = longitude_deg, yend = latitude_deg, colour = dly.mean), size = 1, curvature = 0.1) + 
  geom_point(data=delay_quant, aes(x=longitude_deg, y=latitude_deg),color="red",size=1) +
  scale_colour_gradient2(low="blue", high="red")+
  labs(title = 'Average Departure Delay per Destination - 3rd Quantile', x = "", y = "")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_x_discrete()+
  scale_y_discrete()
```


```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
urlfile<-'https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW/master/sclass.csv'
sclass<-read.csv(url(urlfile))
```

### Exercise 1.2
In this exercise, we used the K-nearest neighbors methodology to build a predictive model for price given mileage, separately for each of two trim levels of the Mercedes S-Class models: 350 and 65 AMG.  First, we divide our data into 2 subgroups, 350 and 65 AMG and define the Root Mean Square Error (RSME) function. The RMSE gives the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are the residuals, when off-sample these deviations are called errors.  

```{r 1.2.1, echo=FALSE}
sclass550 = subset(sclass, trim == '350')
sclass65AMG = subset(sclass, trim == '65 AMG')
rsme = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```

### Sclass 350
First we did a Price vs Mileage plotting of the SClass 350.
The graph indicates a negative relationship between mileage and price.


```{r 1.2.2, echo=FALSE}
plot(price ~ mileage, data = sclass550, main = "Price vs Mileage for SClass350")
```

In supervised learning, we used a training dataset, containing 80% of the outcomes(datapoints) randomly assigned, to train the machine. We then use testing dataset that has the other 20% of the data to predict outcomes.Given the fact that the dataset was randomly splitted and that the sample size is limitted, it is important to note that one might come up with different results every time.

```{r 1.2.3, echo=FALSE}
# Make a train test split
N = nrow(sclass550)
N_train = floor(0.8*N)
N_test = N - N_train

# select random sample to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Formalise training and testing sets
D_train = sclass550[train_ind,]
D_test = sclass550[-train_ind,]

#Arrange as per mileage
D_test = arrange(D_test, mileage)

# Seperate training and testing sets into features (mileage: X) and outcome (price: y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

lm = lm(price ~ poly(mileage, 2), data=D_train)
ypred_lm2 = predict(lm, D_test)

```

We added a linear model to our analysis as a comparison metric as to our hypothesis of how the prices should be falling as per the rise in mileage.

```{r}
ggplot(data = D_test) + 
geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_lm2), color='red') +
labs(title = "Linear Model's price prediction for SClass350")+
theme(plot.title = element_text(hjust = 0.5))
```

Now, we ran K-nearest-neighbors for k, starting from k=3 to the sample size. We faced an error when k=2 given the fact that the KNN function requires a minimum k=3. From here, we use a for loop to our KNN regression, storing all the y-predictors and rsme to evaluate the best possible fit and compare.

The goal here is to find the optimal K which minimizes the RSME in the test set. 
  
```{r 1.2.4, echo=FALSE}

knn_resultpred <- c(1:84)
knn_resultpred = as.data.frame(knn_resultpred)

i=3
for(i in 3:nrow(X_train)) {
  knn_mod = knn.reg(train = X_train, test = X_test, y = y_train, k=i)
  ypred_knn = knn_mod$pred
  knn_resultpred = cbind(knn_resultpred,ypred_knn)
}

vector_1 = c("N",3:332)
vector_1 = sub("^", "K", vector_1)
colnames(knn_resultpred) <- vector_1

knn_resultrsme = data.frame(K=c(),rsme=c())
i=3
for(i in 3:nrow(X_train)) {
  knn_mod = knn.reg(train = X_train, test = X_test, y = y_train, k=i)
  ypred_knn = knn_mod$pred
  rsme_knn = rsme(y_test, ypred_knn) 
  knn_resultrsme =  rbind(knn_resultrsme, c(i,rsme_knn))
}

names(knn_resultrsme) = c("K", "rsme")

```

After obtaining the optimal KKN we plotted the data to visualize our best fit. We do a K vs. RSME graph assessing our best fit which minimizes errors. 

```{r 1.2.5, echo=FALSE}
k_min = with(knn_resultrsme, K[rsme == min(rsme)])
rsme_min = with(knn_resultrsme, rsme[rsme == min(rsme)])
ggplot(data = knn_resultrsme) + 
  geom_point(mapping = aes(x = K, y = rsme), color='lightgrey') + 
  theme_bw(base_size=18) +
  geom_vline(xintercept = k_min, color = "purple", size=1) +
  labs(title = "RSME vs K for SClass350") +
  theme(plot.title = element_text(hjust = 0.5))
```

Then we plotted the prediction generated by the optimal K with the observations, and compared this prediction with other Ks (k=3 and k=75) and the linear model.

```{r 1.2.6, echo=FALSE}
knn_min = knn.reg(train = X_train, test = X_test, y = y_train, k=k_min)
ypred_knn = as.data.frame(knn_min$pred)
names(ypred_knn) = c("ypred_knn")
k_min = with(knn_resultrsme, K[rsme == min(rsme)])

D_test = arrange(D_test, mileage)
ypred_knn = mutate(ypred_knn, mileage = D_test$mileage)
ypred_knn = mutate(ypred_knn, price = D_test$price)
ypred_knn = mutate(ypred_knn, ypred_knn3 = knn_resultpred$K3)
ypred_knn = mutate(ypred_knn, ypred_knn75 = knn_resultpred$K75)
ypred_knn = mutate(ypred_knn, ypred_lm = ypred_lm2)

ggplot(data = ypred_knn) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) +
  geom_path(aes(x = mileage, y = ypred_knn, colour='KNN Min'), size=1.5) +
  geom_path(aes(x = mileage, y = ypred_knn3, colour='KNN 3')) +
  geom_path(aes(x = mileage, y = ypred_knn75, color='KNN 75')) +
  geom_path(aes(x = mileage, y = ypred_lm, color='Linear Model')) +
  scale_color_discrete(name = "Models")+
  labs(title = "Multiple price prediction for SClass350") +
  theme(plot.title = element_text(hjust = 0.5)) 
```

We need to realise that since we are taking randomized samples, the optimal K changes and so would RSME. So our K and RSME would change everytime we take a random sample and run a KNN regression. We found that the optimal K was the following.

```{r showK2, echo=FALSE}
print(k_min)
```

The RSME associated with that K is the following.

```{r showK2, echo=FALSE}
print(rsme_min)
```

Now we can do the same with the 65AMG trim.

### Sclass 65 AMG
Once again we started by plotting a Price vs Mileage graph of the SClass 65AMG.
The graph indicates the same negative relationship between mileage and price.

```{r 1.2.9, echo=FALSE}
# plot the data
plot(price ~ mileage, data = sclass65AMG, main = "Price vs Mileage for SClass65AMG")
```

We again randomly splitted the data (this time the for the 65AMG model) into two groups. One is "training set", and the other is "test set", with the same shares as before. We ran the same linear model as before in order to make comparisons with the different fitted values.

```{r 1.2.10, echo=FALSE}
M = nrow(sclass65AMG)
M_train = floor(0.8*M)
M_test = M - M_train

# select random sample to include in the training set
train_ind2 = sample.int(M, M_train, replace=FALSE)

# Formalise training and testing sets
E_train = sclass65AMG[train_ind2,]
E_test = sclass65AMG[-train_ind2,]


#housekeeping, just arrange as per mileage
E_test = arrange(E_test, mileage)

# Seperate training and testing sets into features (mileage: X) and outcome (price: y)
X2_train = select(E_train, mileage)
y2_train = select(E_train, price)
X2_test = select(E_test, mileage)
y2_test = select(E_test, price)

lm2 = lm(price ~ poly(mileage, 2), data=E_train)
ypred_lm22 = predict(lm2, E_test)
```

The linear model corroborates the idea that prices should be falling mileage increases.

```{r 1.2.11, echo=FALSE}
ggplot(data = E_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_lm22), color='blue') +
  labs(title = "Linear Model's price prediction for SClass65AMG")+
  theme(plot.title = element_text(hjust = 0.5))
```

Again, for the 65AMG, we run K-nearest-neighbors for k, starting from k=3 to the sample size. Just like before, we used a for loop to our KNN regression and a y-predictor to find the best possible fit, with the goal to find the optimal K which minimizes the RSME.
We plotted a K vs. RSME graph assessing out best fit which minimizes errors.

```{r 1.2.12, echo=FALSE}
knn_resultpred2 <- c(1:59)
knn_resultpred2 = as.data.frame(knn_resultpred2)

i=3
for(i in 3:nrow(X2_train)) {
  knn_mod2 = knn.reg(train = X2_train, test = X2_test, y = y2_train, k=i)
  ypred_knn2 = knn_mod2$pred
  knn_resultpred2 = cbind(knn_resultpred2,ypred_knn2)
}

vector_2 = c("N",3:233)
vector_2 = sub("^", "K", vector_2)
colnames(knn_resultpred2) <- vector_2

knn_resultrsme2 = data.frame(K=c(),rsme=c())
i=3
for(i in 3:nrow(X2_train)) {
  knn_mod2 = knn.reg(train = X2_train, test = X2_test, y = y2_train, k=i)
  ypred_knn2 = knn_mod2$pred
  rsme_knn2 = rsme(y2_test, ypred_knn2) 
  knn_resultrsme2 =  rbind(knn_resultrsme2, c(i,rsme_knn2))
}

names(knn_resultrsme2) = c("K", "rsme")
k_min2 = with(knn_resultrsme2, K[rsme == min(rsme)])
rsme_min2 = with(knn_resultrsme2, rsme[rsme == min(rsme)])

ggplot(data = knn_resultrsme2) + 
  geom_point(mapping = aes(x = K, y = rsme), color='lightgrey') + 
  theme_bw(base_size=18) +
  geom_vline(xintercept = k_min2, color = "purple", size=1) +
  labs(title = "RSME vs K for SClass65AMG") +
  theme(plot.title = element_text(hjust = 0.5))
```
Then just like before, we plot k=3 and k=75. So in total we have K=3, k=75, minimizing K and the linear model. 
```{r 1.2.13, echo=FALSE}
knn_min2 = knn.reg(train = X2_train, test = X2_test, y = y2_train, k=k_min2)
ypred_knn2 = as.data.frame(knn_min2$pred)
names(ypred_knn2) = c("ypred_knn2")

E_test = arrange(E_test, mileage)
ypred_knn2 = mutate(ypred_knn2, mileage = E_test$mileage)
ypred_knn2 = mutate(ypred_knn2, price = E_test$price)
ypred_knn2 = mutate(ypred_knn2, ypred_knn3 = knn_resultpred2$K3)
ypred_knn2 = mutate(ypred_knn2, ypred_knn75 = knn_resultpred2$K75)
ypred_knn2 = mutate(ypred_knn2, ypred_lm = ypred_lm22)

ggplot(data = ypred_knn2) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) +
  geom_path(aes(x = mileage, y = ypred_knn2, color='KNN Min'), size=1.5) +
  geom_path(aes(x = mileage, y = ypred_knn3, color='KNN 3')) +
  geom_path(aes(x = mileage, y = ypred_knn75, color='KNN 75')) +
  geom_path(aes(x = mileage, y = ypred_lm, color='Linear Model')) +
  scale_color_discrete(name = "Models")+
  labs(title = "Multiple price prediction for SClass65AMG") +
  theme(plot.title = element_text(hjust = 0.5))
```

Like mentioned before, note that as we are taking random samples, the optimal K changes and so would RSME everytime we take a different sample and run a KNN regression. 

We found that the optimal K was the following.

```{r showK2, echo=FALSE}
print(k_min2)
```

The RSME associated with that K is the following.

```{r showK2, echo=FALSE}
print(rsme_min2)
```

We can see that the optimal k value is larger in subgroup 65 AMG than sclass  Sclass 350. However, we can't assess whether it is due to a different pattern or a difference in the randomization process.

In addition, samples in 350 are more dispersed than those in 65 AMG. The large variance of sample can be aruged as giving a better 'data quality' and this could be one factor which attributes to a different value of k for the 350 sub-class.
